---
title: "R Notebook"
output: html_notebook
---

A Titanic katasztrófa túlélésének predikciója

Adatok beolvasása, előkészítése:

```{r}
library(readr)
gender_submission <- read.csv("gender_submission.csv")
View(gender_submission)
train <- read_csv("train.csv", 
     col_types = cols(
         Sex = col_factor(levels = c("male", "female")),
         Age = col_double(), 
         Embarked = col_factor(levels = c("C", "S", "Q")), 
         Parch = col_integer(), 
         Pclass = col_factor(levels = c("3", "2", "1"), ordered = TRUE), 
         SibSp = col_integer(), 
         Survived = col_integer()))
test <- read_csv("test.csv", 
     col_types = cols(
         Sex = col_factor(levels = c("male", "female")),
         Age = col_double(), 
         Embarked = col_factor(levels = c("C", "S", "Q")), 
         Parch = col_integer(), 
         Pclass = col_factor(levels = c("3", "2", "1"), ordered = TRUE), 
         SibSp = col_integer()))
View(test)
```


```{r}
str(train)
```


```{r}
library(xgboost)
library(caret)

# adatok one-hot kódolása

newdata <- dummyVars("~ Pclass + Sex + Embarked", data = train)
train_data <- data.frame(predict(newdata, newdata = train))
train_data$Age <- train$Age
train_data$SibSp <- train$SibSp
train_data$Parch <- train$Parch
train_data$Fare <- train$Fare
train_data$Family <- train$SibSp + train$Parch
View(train_data)

# xgboost modellhez DMatrix szükséges, mind a tanuló, mind a teszthalmazhoz

train_dmatrix <- xgb.DMatrix(label = train$Survived, data = as.matrix(train_data))

newtest <- dummyVars("~ Pclass + Sex + Embarked", data = test)
test_data <- data.frame(predict(newtest, newdata = test))
test_data$Age <- test$Age
test_data$SibSp <- test$SibSp
test_data$Parch <- test$Parch
test_data$Fare <- test$Fare
test_data$Family <- test$SibSp + test$Parch
View(test_data)

test_dmatrix <- xgb.DMatrix(data = as.matrix(test_data))

# kezdetleges xgboost

xgbmodel <- xgboost(data = train_dmatrix, max.depth = 2, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")

# paraméter tuningolás (randomsearch) -> túl sok paraméter, globális optimum megtalálása túl "költséges" -> a véletlent hívjuk segítségül

start_time <- Sys.time()

best_param <- list()
best_acc <- 0
best_acc_index <- 0

for (iter in 1:1000) {
  # a modell paramétereinek listája, ezeket szeretnénk tuningolni
  param_list <- list(objective = "binary:logistic",  #binary:logistic, binary:hinge
                eval_metric = c("error"),      
                max_depth = sample(3:10, 1),
                eta = runif(1, .01, .3),   
                subsample = runif(1, .6, .9),
                colsample_bytree = runif(1, .6, .9), 
                min_child_weight = sample(5:10, 1), 
                max_delta_step = sample(1:10, 1),
                base_score = runif(1, .3, .6)
                )
  xgb_cv <- xgb.cv(data = train_dmatrix, booster = "gbtree", params = param_list,  
                 nfold = 10, nrounds = 20,
                 verbose = F, early_stopping_rounds = 20, maximize = FALSE,
                 stratified = T)

  max_acc_index  <-  xgb_cv$best_iteration
  max_acc <- 1 - xgb_cv$evaluation_log[xgb_cv$best_iteration]$test_error_mean

  #print(max_acc)
  #print(xgb_cv$evaluation_log[xgb_cv$best_iteration])

  # keressük a legpontosabb paramétereket
  if (max_acc > best_acc) {
    best_acc <- max_acc
    best_acc_index <- max_acc_index
    best_param <- param_list
  }
  
  if (max_acc < .7){
    print(param_list)
  }
}

end_time <- Sys.time()

print(end_time - start_time)

print(best_acc)
print(best_param)
```

