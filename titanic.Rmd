---
title: "R Notebook"
output: html_notebook
---

A Titanic katasztrófa túlélésének predikciója
=============================================


Könyvtárak:
```{r}
  library(readr)
  library(mice)
  library(caret)
  library(splines)
  library(glmnet)
  library(nnet)
  library(rpart)
  library(randomForest)
  library(gbm)
  library(caretEnsemble)
  library(ModelMetrics)
  library(plotmo)
  library(plyr)
  library(ggplot2)
  library(gmodels)
  library(xgboost)
  library(earth)
  library(e1071)
  library(hrbrthemes)
  library(rattle)
```

Adatok beolvasása, előkészítése:
```{r}
  train <- read_csv("train.csv", 
       col_types = cols(
           Sex = col_factor(levels = c("male", "female")),
           Age = col_double(), 
           Embarked = col_factor(levels = c("C", "S", "Q")), 
           Parch = col_integer(), 
           Pclass = col_factor(levels = c("3", "2", "1"), ordered = TRUE), 
           SibSp = col_integer(), 
           Survived = col_integer()))
  test <- read_csv("test.csv", 
       col_types = cols(
           Sex = col_factor(levels = c("male", "female")),
           Age = col_double(), 
           Embarked = col_factor(levels = c("C", "S", "Q")), 
           Parch = col_integer(), 
           Pclass = col_factor(levels = c("3", "2", "1"), ordered = TRUE), 
           SibSp = col_integer()))
  
  traink <- dummyVars("~ Sex + Embarked", data = train)
  traink <- data.frame(predict(traink, newdata = train))
  traink$Pclass <- train$Pclass
  traink$Age <- train$Age
  traink$SibSp <- train$SibSp
  traink$Parch <- train$Parch
  traink$Fare <- train$Fare
  traink$Survived <- train$Survived
  set.seed(1912)
  traink <- complete(mice(traink, method="rf", m=1))
  
  y <- traink$Survived
  X <- model.matrix(Survived~., data=traink)
  X <- X[,c("Sex.male", "Sex.female", "Embarked.C", "Embarked.S", "Embarked.Q", "Pclass.L", "Pclass.Q", "Age", "SibSp", "Parch", "Fare")]
  
  sample1 <- sort(sample(c(1:891), size=600, replace=F), decreasing=F)
  X1 <- X[sample1,]
  X2 <- X[-sample1,]
  y1 <- y[sample1]
  y2 <- y[-sample1]
  
  Xvesz<-data.frame(cbind(X1,Survived=y1))
```

```{r}
  CrossTable(traink$Survived, traink$Sex.female)
  ggplot(data=traink, aes(Age))+geom_histogram()
  ggplot(data=traink, aes(Fare))+geom_histogram()
  ggplot(data=traink, aes(Survived))+geom_bar()
  ggplot(data=traink, aes(x=as.factor(Survived), y=Age, color=Survived))+geom_boxplot()
```

Lasso modell:
```{r}
  Lassov<-cv.glmnet(X1,y1)
  plot(Lassov)
  coef(Lassov, Lassov$lambda.1se)
  
  lambda_seq <- 10^seq(2, -4, by = -.5)
  Lasso <- cv.glmnet(X1, y1, alpha = 1, lambda = lambda_seq, nfolds = 5)
  best_lam <- Lasso$lambda.min
  lasso_best <- glmnet(X1, y1, alpha = 1, lambda = best_lam)
  predlasso <- predict(lasso_best, s = best_lam, newx = X2)
  predlasso
  best_lam
  
  acclas=rep(NA, 101)
  for(i in (1:101)){
    acclas[i]=+i/100
  }

  acc <- rep(NA, length(acclas)) 
  for(i in 1:101){
    pred <- predlasso > acclas[i]
    acc[i] <- mean(pred == y2)
  }
  acc
  
  binpredlasso<-predlasso>=0.58
  mean(binpredlasso==y2)
```

Spline-függvényes megközelítés:
```{r}
  Xvesz<-data.frame(cbind(X1,Survived=y1))
  spline_reg=lm(Survived~., data=Xvesz)
  summary(spline_reg)

  plotmo(spline_reg)
  predspline<-predict(spline_reg, data.frame(X2))
  predspline
  
  accspl1=rep(NA, 101)
  for(i in (1:101)){
    accspl1[i]=+i/100
  }

  accspl2 <- rep(NA, length(accspl1)) 
  for(i in 1:101){
    pred <- predspline > accspl1[i]
    accspl2[i] <- mean(pred == y2)
  }
  accspl2
  
  binpredspline<-predspline>=0.59
  mean(binpredspline==y2)
```

MARS-modell:
```{r}
  MARS<-earth(Survived~., data=Xvesz, degree=2)
  summary(MARS)
  plotmo(MARS)
  
  predmars<-predict(MARS, X2)
  predmars
  
  accmar1=rep(NA, 101)
  for(i in (1:101)){
    accmar1[i]=+i/100
  }

  accmar2 <- rep(NA, length(accmar1)) 
  for(i in 1:101){
    pred <- predmars > accmar1[i]
    accmar2[i] <- mean(pred == y2)
  }
  accmar2
  
  binpredmars<-predmars>=0.58
  mean(binpredmars==y2)
```

SVM-modell:
```{r}
  SVM <- svm(as.factor(Survived)~., data=Xvesz, scale=TRUE)
  plot(SVM, Age~Fare, data=Xvesz)
  
  racs <- expand.grid(C=2^(-4:4), sigma=10^(-4:4))
  finomh <- train(as.factor(Survived)~., data=Xvesz, method="svmRadial", tuneGrid=racs)
  finomh_eredmeny <- finomh$results
  library(ggplot2)
  ggplot(finomh_eredmeny, aes(x=log2(C), y=log10(sigma), fill=Accuracy))+geom_tile()
  
   #A C=0, sigma=10^-1 környékén finomítjuk
   racs2 <- expand.grid(C=2^seq(from=0, to=6, by=0.5), sigma=10^seq(from=-3, to=0, by=0.5))
   finomh2 <- train(as.factor(Survived)~., data=Xvesz, method="svmRadial", tuneGrid=racs2)
   finomh_eredmeny2 <- finomh2$results
   ggplot(finomh_eredmeny2, aes(x=log2(C), y=log10(sigma), fill=Accuracy), col="red")+geom_tile()+scale_fill_gradient(low="white", high="darkgreen")
```

Véletlen erdő:
```{r}
  keresztval <- trainControl(method="cv", number=5)
  fa <- train(as.factor(Survived)~., data=traink, method="rpart", trControl=keresztval)
 fa # cp=0.01461988 esetén Accuracy=0.8002134, Kappa=0.5708988
 #plot(fa$finalModel)
 #text(fa$finalModel)
 fancyRpartPlot(fa$finalModel)
```

```{r}
  mtry <- data.frame(mtry=1:10)
  erdo <- train(as.factor(Survived)~., data=traink, method="rf", trControl=keresztval, ntree=30, tuneGrid=mtry)
  erdo # mtry=4 esetén Accuracy=0.8440274, Kappa=0.6614112
  plot(erdo$finalModel) # ez nem rendes erdőrajz, de olyat careten belül nem találtam
  
  erdo$finalModel
```

```{r}
  gboost <- train(as.factor(Survived)~., data=traink, method="gbm", trControl=keresztval)
  gboost # n.trees = 150, interaction.depth = 2, shrinkage = 0.1 and n.minobsinnode = 10 esetén Accuracy=0.8249262, Kappa=0.6225874
```

```{r}
  hyper <- data.frame(n.trees = 150, interaction.depth = 2, shrinkage = 0.1, n.minobsinnode = 10)
  gboost_best <- train(as.factor(Survived)~., data=traink, method="gbm", trControl=keresztval, tuneGrid=hyper)
  mtry <- data.frame(mtry=4)
  erdo_best <- train(as.factor(Survived)~., data=traink, method="rf", trControl=keresztval, ntree=30, tuneGrid=mtry)
  summary(gboost_best)
  gboost_best # Accuracy=0.8248481, Kappa=0.6215871
  summary(erdo_best)
  erdo_best # Accuracy=0.8384265, Kappa=0.6501237
```

Változók fontossága, becslés
```{r}
  fontos_gboost <- varImp(gboost_best)
  fontos_gboost <- fontos_gboost$importance
  plot(varImp(gboost_best))

  fontos_erdo <- varImp(erdo_best)
  fontos_erdo <- fontos_erdo$importance
  plot(varImp(erdo_best))
```

==================================================
Ezek átírandók!!!
```{r}
  becsultgb <- predict(gboost_best, testk)
  CrossTable(becsultgb,gender_submission$Survived)
```

```{r}
becsulterdo <- predict(erdo_best, testk)
  CrossTable(becsulterdo,gender_submission$Survived)
```

Benedeké:
```{r}
  library(readr)
  gender_submission <- read.csv("gender_submission.csv")
  View(gender_submission)
  train <- read_csv("train.csv", 
       col_types = cols(
           Sex = col_factor(levels = c("male", "female")),
           Age = col_double(), 
           Embarked = col_factor(levels = c("C", "S", "Q")), 
           Parch = col_integer(), 
           Pclass = col_factor(levels = c("3", "2", "1"), ordered = TRUE), 
           SibSp = col_integer(), 
           Survived = col_integer()))
  test <- read_csv("test.csv", 
       col_types = cols(
           Sex = col_factor(levels = c("male", "female")),
           Age = col_double(), 
           Embarked = col_factor(levels = c("C", "S", "Q")), 
           Parch = col_integer(), 
           Pclass = col_factor(levels = c("3", "2", "1"), ordered = TRUE), 
           SibSp = col_integer()))
  View(test)
```

```{r}
  # adatok one-hot kódolása
  
  newdata <- dummyVars("~ Pclass + Sex + Embarked", data = train)
  train_data <- data.frame(predict(newdata, newdata = train))
  train_data$Age <- train$Age
  train_data$SibSp <- train$SibSp
  train_data$Parch <- train$Parch
  train_data$Fare <- train$Fare
  train_data$Family <- train$SibSp + train$Parch
  View(train_data)
  
  # xgboost modellhez DMatrix szükséges, mind a tanuló, mind a teszthalmazhoz
  
  train_dmatrix <- xgb.DMatrix(label = train$Survived, data = as.matrix(train_data))
  
  
  newtest <- dummyVars("~ Pclass + Sex + Embarked", data = test)
  test_data <- data.frame(predict(newtest, newdata = test))
  test_data$Age <- test$Age
  test_data$SibSp <- test$SibSp
  test_data$Parch <- test$Parch
  test_data$Fare <- test$Fare
  test_data$Family <- test$SibSp + test$Parch
  View(test_data)
  
  test_dmatrix <- xgb.DMatrix(data = as.matrix(test_data))
  
  # kezdetleges xgboost
  
  xgbmodel <- xgboost(data = train_dmatrix, max.depth = 2, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")
  
  # paraméter tuningolás (randomsearch) -> túl sok paraméter, globális optimum megtalálása túl "költséges" -> a véletlent hívjuk segítségül
  
  start_time <- Sys.time()
  
  best_param <- list()
  best_acc <- 0
  best_acc_index <- 0
  
  for (iter in 1:1000) {
    # a modell paramétereinek listája, ezeket szeretnénk tuningolni
    param_list <- list(objective = "binary:logistic",  #binary:logistic, binary:hinge
                  eval_metric = c("error"),      
                  max_depth = sample(3:10, 1),
                  eta = runif(1, .01, .3),   
                  subsample = runif(1, .6, .9),
                  colsample_bytree = runif(1, .6, .9), 
                  min_child_weight = sample(5:10, 1), 
                  max_delta_step = sample(1:10, 1),
                  base_score = runif(1, .3, .6)
                  )
    xgb_cv <- xgb.cv(data = train_dmatrix, booster = "gbtree", params = param_list,  
                   nfold = 10, nrounds = 20,
                   verbose = F, early_stopping_rounds = 20, maximize = FALSE,
                   stratified = T)
  
    max_acc_index  <-  xgb_cv$best_iteration
    max_acc <- 1 - xgb_cv$evaluation_log[xgb_cv$best_iteration]$test_error_mean
  
    #print(max_acc)
    #print(xgb_cv$evaluation_log[xgb_cv$best_iteration])
  
    # keressük a legpontosabb paramétereket
    if (max_acc > best_acc) {
      best_acc <- max_acc
      best_acc_index <- max_acc_index
      best_param <- param_list
    }
    
    if (max_acc < .7){
      print(param_list)
    }
  }
  
  end_time <- Sys.time()
  
  print(end_time - start_time)
  
  print(best_acc)
  print(best_param)
```
